---
title: 'Analysis 2: Joining Cholesterol with Crimes and Web Scraping Wikipedia'
author: "Anthony"
date: '06/08/2021'
output:
  pdf_document: default
  html_document: default
---

# Instructions

**Overview:** For each question, show your R code that you used to answer each question in the provided chunks. When a written response is required, be sure to answer the entire question in complete sentences outside the code chunks. When figures are required, be sure to follow all requirements to receive full credit. Point values are assigned for every part of this analysis.

**Helpful:** Make sure you knit the document as you go through the assignment. Check all your results in the created HTML or PDF file.

**Submission:** Submit via an electronic document on Sakai. Must be submitted as an PDF or an HTML file generated in RStudio. 

```{r setup, include=F}
options(scipen=999)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
```

# Introduction

Does high cholesterol lead to high crime rates? Probably not, but web scraping will definitely lead to lower crime rates. This data analysis assignment is separated into three parts which cover material from the lectures on tidy data, joins, and webscraping. In Part 1, you will demonstrate the basic concept of joins by connecting relational data involving a cholesterol study. For this segment, `pivot_longer` and `pivot_wider` will be utilized to create a single tidy dataset ready for analysis. In Part 2, we will join all 5 datasets from the lecture series on web scraping. Part 3 will require an understanding of web scraping to import a table found on Wikipedia directly into R. The following R code reads in all datasets required for this assignment.

```{r,message=F}
# Data for Part 1
CHOL1=read_csv("Cholesterol.csv")
CHOL2=read_csv("Cholesterol2.csv")

# Data for Part 2
VIOLENT=read_csv("FINAL_VIOLENT.csv")
ZIP=read_csv("FINAL_ZIP.csv")
STATE_ABBREV=read_csv("FINAL_STATE_ABBREV.csv")
CENSUS=read_csv("FINAL_CENSUS.csv")
S_VS_D=read_csv("FINAL_SAFE_VS_DANGEROUS.CSV")

```




# Assignment

## Part 1: Cholesterol Experiment

The data frame `CHOL1` contains experimental results from randomly assigning *18* people to one of two competing margarine brands "A" and "B". The cholesterol of these patients was measured once before using the margarine brand, once after *4* weeks with the margarine brand, and then again after *8* weeks with the margarine brand. Researchers want to see if there is benefit of these brands of margarine on reducing an individual's cholesterol and want to determine if there is a statistically significant difference between the two competing brands. 

```{r}
CHOL1
CHOL2
```
 
### Q1 *(3 Points)*

Start by examing the tables `CHOL1` and `CHOL2` and answering the following questions with *Yes* or *No* responses.

Is the variable `ID` in `CHOL1` a primary key? 

Answer *(1 Point)*: Yes

Is the variable, `Margarine` in `CHOL1` a primary key?

Answer *(1 Point)*: No

Is the variable, `Brand` in `CHOL2` a primary key? 

Answer *(1 Point)*: No

### Q2 *(2 Points)*

In a new data frame called `CHOL1a` based on `CHOL1`, rename the variables `After4weeks` and `After8weeks` to nonsynctactic variable names `4` and `8`, respectively. Use `names(CHOL1a)` to display this modification.

```{r}
#
CHOL1a <- CHOL1 %>%
  rename("4" = After4weeks, "8" = After8weeks)
names(CHOL1a)
CHOL1a
```

### Q3 *(4 Points)*

Use the `pivot_longer()` function or `gather()` function on `CHOL1a` to create a new numeric variable called `Week` that contains numeric values *4* or *8* and a new numeric variable called `Response` that contains the Cholesterol after the corresponding number of weeks. Create a new data frame called `CHOL1b` with these modifications and use `str(Chol1b)` to show that both variables have been created correctly and are indeed numeric (an integer variable is a specific type of numeric variable).

```{r}
#
CHOL1a
CHOL1b <- CHOL1a %>%
  pivot_longer(3:4, names_to = "Week", values_to = "Response")
CHOL1b
str(CHOL1b)
```

### Q4 *(4 Points)*

Now working with `CHOL2`, we want to spread the variable `Statistic` across multiple columns. Do this in a new data frame called `CHOL2a` and use `print(CHOL2a)` to display the modified complete table.

```{r}
#
names(CHOL2)
CHOL2a <- CHOL2 %>%
  separate("Brand\tStatistic\tValue", c("Brand", "Statistics", "Value"), "\t")
CHOL2a
```

### Q5 *(3 Points)*

Start by examing the tables `CHOL1b` and `CHOL2a` and answering the following questions with *Yes* or *No* responses.

Is the variable `ID` in `CHOL1b` a primary key? 

Answer *(1 Point)*: No

Is the variable, `Margarine` in `CHOL1b` a primary key?

Answer *(1 Point)*: No

Is the variable, `Brand` in `CHOL2a` a primary key? 

Answer *(1 Point)*: No

### Q6 *(4 Points)*

Get the nutritional facts of the different margarine brands in `CHOL2a` into the experimental results found in `CHOL1b` using a join. Create a new data frame named `CHOL.COMBINED` and display the table using `head(CHOL.COMBINED)`. This final data frame should contain *36* observations and *10* variables.

```{r}
#
CHOL.COMBINED <- CHOL1b %>%
  rename(Brand = "Margarine") %>%
  full_join(CHOL2a, by = "Brand") %>% 
  pivot_wider(names_from = "Statistics", values_from = "Value")
head(CHOL.COMBINED)
dim(CHOL.COMBINED)
```


## Part 2: Linking Important Information to 2017 Violent Crimes Data

In the zipped folder, there are *5* CSV files. In this section, we are going to merge all of that data into one object called `FINAL.VIOLENT`. 

### Q1 *(2 Points)*

The dataset `S_VS_D` contains a variable `CLASS` where "S=Safe" and "D=Dangerous" according to the article *[These Are the 2018 Safest and Most Dangerous States in the U.S](https://www.securitysales.com/fire-intrusion/2018-safest-most-dangerous-states-us/)* by [Steve Karantzoulidis](https://www.securitysales.com/author/stevek/). We seek to compare the violent crime statistics for states not in this list. Use a filtering join to create a new data frame called `VIOLENT2` that only contains violent crime statistics from the states not represented in the data frame `S_VS_D`.  Use `str(VIOLENT2)` to display the variables and the dimensions of `VIOLENT2`.


```{r}
#

S_VS_D_2 <- S_VS_D %>%
  rename(State = "STATE")
VIOLENT2 <- VIOLENT %>%
  anti_join(S_VS_D_2, by = "State")
str(VIOLENT2)
dim(VIOLENT2)
```

### Q2 *(4 Points)*

Start by creating a new data set called `VIOLENT3` based on `VIOLENT2` that fixes some problems in the variable `City`. Specifically, we would like to change "Louisville Metro" to "Louisville". 

Next, create a new data frame named `VIOLENT4` that connects the population change and density measures from 2019 contained in `CENSUS` to the cities and states in `VIOLENT3`. Use `head(VIOLENT4)` to give a preview of the new merged dataset.

Finally, in a complete sentence, identify any location(s) (Cities and States) missing violent crime information.

Code and Output *(2 Points)*:
```{r}
#
VIOLENT2$City[VIOLENT2$City == "Louisville Metro"] <- "Louisville"
VIOLENT3 <- VIOLENT2

VIOLENT4 <- CENSUS %>%
  rename(City = "Name") %>%
  full_join(VIOLENT3, by = "City") %>%
  select(-State.y, State = State.x)
VIOLENT4
head(VIOLENT4, 10)

VIOLENT_non_NA <- VIOLENT4 %>%
  filter(Population > 0| Total > 0 | Murder > 0 | Rape > 0 |  Robbery > 0 | Assault > 0)

VIOLENT_NA <-VIOLENT4 %>%
  anti_join(VIOLENT_non_NA, by = "City") %>%
  arrange(State)
VIOLENT_NA
```

  
Answer *(2 Points)*: In the dataset VIOLENT_NA, various cities, especially those in Alabama, Texas, Arizona, California, Colorado, Connecticut, Florida, Illinois, are missing datavalues for crime. 


### Q3 *(6 Points)*

Either ambitiously using one step or less-ambitiously using multiple steps add the longitude and latitude information provided in `ZIP` to the cities and states in `VIOLENT4`. You will need to use `STATE_ABBREV` data to link these two data frames. Your final data frame named `FINAL.VIOLENT` should contain all of the information in `VIOLENT4` along with the variables `lat` and `lon` from `ZIP`. There should be **no** state abbreviations in `FINAL.VIOLENT` since this information is redundant. Use `str(FINAL.VIOLENT)` to demonstrate that everything worked as planned. 

In `FINAL.VIOLENT` identify what cities are missing latitude and longitude. Closely, inspect both the `ZIP` and `VIOLENT4` data frames. Report the location(s) missing geographical information and explain in complete sentences why this happened. 

Finally, challenge yourself and attempt to fix this problem in a new data frame called `FINAL.VIOLENT.FIX`. Use a combination of `str()` and `filter()` to only display the data in `FINAL.VIOLENT.FIX` for the location(s) that `FINAL.VIOLENT` was missing latitude and longitude. Do this in the second code chunk below.

Code and Output *(4 Points)*:
```{r}
#
ZIP_v2 <- ZIP %>%
  rename(City = "city") %>%
  full_join(STATE_ABBREV, by = "state")


FINAL.VIOLENT <- VIOLENT4 %>%
  full_join(ZIP_v2, by = "City") %>%
  rename(State = "State.y", State_code = "state") %>%
  arrange(City)


Missing_Long_Lat <- FINAL.VIOLENT %>%
  filter(is.na(lon) | is.na(lat)) %>%
  arrange(City)
Missing_Long_Lat
```

Answer *(1 Points)*: In the Missing_Long_lat dataset, there are 13 observations with either missing lat or lon values, because the 13 cities in the VIOLENT4 dataset do not have a corresponding observation of the same City value -- the primary key -- in the ZIP dataset.

Code and Output *(1 Point)*:
```{r}
#
str(FINAL.VIOLENT$lat)
str(FINAL.VIOLENT$lon)

FINAL.VIOLENT.FIX <- FINAL.VIOLENT %>%
  filter(is.na(lon) | is.na(lat)) %>%
  select(-"State") %>%
  rename(State = "State.x") %>%
  select(City, State, lat, lon, everything())
FINAL.VIOLENT.FIX
```

## Part 3: Web Scraping a Table From Wikipedia

Wikipedia contains a rough estimate of a billion tables. Search through Wikipedia pages and identify an article, completely unrelated to crimes data, that contains an HTML table that has at least *5* rows and *3* columns. You will be required to web scrape the table into a data frame or tibble into R. This portion will require a minor knowledge of the `rvest` package. Utilize information from the web scraping lectures and tutorials to assist you with this.

### Q1 *(4 Points)*

What is the URL of the Wikipedia page you plan on webscraping (Knit the Document and Check the Hyperlink)? 

Answer *(2 Points)*: [Fastest recorded tennis serves](https://en.wikipedia.org/wiki/Fastest_recorded_tennis_serves)

In *2* to *5* sentences, Identify and describe the specific table you plan on web scraping. State the variables in *1* of the sentences.

Answer *(2 Points)*: The table I plan to web scrape is highest recorded men's and women's tennis serves, but they have different tables, for men and women. There are multiple variables, such as Player(Player name), speed(in both mph and km/h), Event(Tournament that serve occured), Type(Singles or Doubles), and Round(which stage of the tournament). The list is potentially missing some entries, due to the fact that some serves are not verifiable using modern technology. Also, the serves listed are only the personal best for the players. Some players could have multiple entries, but are only limited to one--their personal best-- on the table.


### Q2 *(4 Points)*

Utilize the functions `read_html()` and `html_table()` to web scrape the specific table you described above. Internet access will be required for these functions to work. Create an R data frame named `DATA` which contains the information from the Wikipedia table. All code should be contained in the R code chunk below. Finally, use the `print()` function to display the table to demonstrate that everything worked as planned. The variable names and the content should match the table on the Wikipedia page you chose exactly. You are not required to perform any cleaning of this data. As long as the content of the table you describe matches `DATA`, then you are good. Don't worry if the table bleeds over multiple pages.

```{r}
#
URL.SERVES = "https://en.wikipedia.org/wiki/Fastest_recorded_tennis_serves"
DATA = URL.SERVES %>%
          read_html() %>%
          html_nodes(css=".headerSort , td , .Template-Fact span") %>%
          html_text
DATA
```

